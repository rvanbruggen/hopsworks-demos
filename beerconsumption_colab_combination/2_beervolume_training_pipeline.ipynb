{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rvanbruggen/rix-hopsworks-demos/blob/main/Beerconsumption_Colab/2_beervolume_training_pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BeerVolume Prediction Demo - Training Pipeline (2)\n",
        "\n",
        "This is the seocnd part of a short demonstration of how you can use Hopsworks for creating a Machine Learning System that creates predictions. The hypothetical use case is that of a **bar owner** who would want to predict how much beer will be consumed in his bar, based on past trends and behaviour.\n",
        "\n",
        "![](https://lh3.googleusercontent.com/blogger_img_proxy/ALY8t1uqu0YUTdfoFJYGV2r9a2iHEewpP3daVa9J3qzCzV3rZm8EX8YyhHhOHbfG450AhHYQXu6Hgf8pj2fTpSzg4uio4X_qv9TTEfMnEtO6rYLevnGBxF6sO97tGeYyzaAkGSyVBnw8WtWS1P_2RLY=s0-d)\n",
        "\n",
        "In this second part of the Machine Learning system that we are creating, we are going to focus on the **Training Pipeline** required to make the predictions that we want."
      ],
      "metadata": {
        "id": "H0imbqwQze3O"
      },
      "id": "H0imbqwQze3O"
    },
    {
      "cell_type": "markdown",
      "id": "83dd6e7e",
      "metadata": {
        "id": "83dd6e7e"
      },
      "source": [
        "## <span style=\"color:#ff5f27\">üìù Code Library Imports and installations</span>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Colab / Gdrive integration\n",
        "In order to work with files from Gdrive, we need to include the following:"
      ],
      "metadata": {
        "id": "IodQcGnnnaiq"
      },
      "id": "IodQcGnnnaiq"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!cp '/content/drive/MyDrive/Colab Notebooks/Beervolume predictions/beervolume.py' .\n",
        "!cp '/content/drive/MyDrive/Colab Notebooks/Beervolume predictions/averages.py' .\n",
        "!cp '/content/drive/MyDrive/Colab Notebooks/Beervolume predictions/functions.py' ."
      ],
      "metadata": {
        "id": "9h5-OXXjCJ_g"
      },
      "execution_count": null,
      "outputs": [],
      "id": "9h5-OXXjCJ_g"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Installing Hopsworks libraries\n",
        "We also need to install the Hopsworks libraries so that we can use these during out notebook exploration"
      ],
      "metadata": {
        "id": "oep_k4bUq3_3"
      },
      "id": "oep_k4bUq3_3"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U hopsworks --quiet"
      ],
      "metadata": {
        "id": "8jJ4SAyxq4pA"
      },
      "execution_count": null,
      "outputs": [],
      "id": "8jJ4SAyxq4pA"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Installing Supporting libraries\n",
        "We also need to install the Kaleido libraries so that we can use these during our notebook exploration:"
      ],
      "metadata": {
        "id": "-02JmmZD1IWZ"
      },
      "id": "-02JmmZD1IWZ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c5b0a68",
      "metadata": {
        "id": "5c5b0a68"
      },
      "outputs": [],
      "source": [
        "!pip install -U kaleido # For Plotly Image export"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Other imports\n",
        "We are importing a number of other libaries, among which several classes from  `beervolume.py` and `averages.py`, into this notebook, as we will use it later on."
      ],
      "metadata": {
        "id": "pVf3hoBmnjOQ"
      },
      "id": "pVf3hoBmnjOQ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8355be0e",
      "metadata": {
        "id": "8355be0e"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import xgboost as xgb\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import os\n",
        "import joblib\n",
        "from beervolume import plot_prediction_test\n",
        "from functions import predict_id\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9dbea0ad",
      "metadata": {
        "id": "9dbea0ad"
      },
      "source": [
        "## <span style=\"color:#ff5f27\">üîÆ Connect to Hopsworks Feature Store </span>\n",
        "\n",
        "As we have already installed Hopsworks in a previous cell, all we now need to do is import the library into this notebook, and start establishing the connection to the Hopsworks feature store."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e499f528",
      "metadata": {
        "id": "e499f528"
      },
      "outputs": [],
      "source": [
        "import hopsworks\n",
        "\n",
        "project = hopsworks.login()\n",
        "\n",
        "fs = project.get_feature_store()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since we have already created the necessary feature groups in our previous notebook, we can now just import these into our training pipeline. Remember we had two feature groups:\n",
        "1. a `beervolume` feature group that contained the synthetic data that we had created,\n",
        "2. a `beervolume_averages` feature group that contained a number of _engineered_ features that we derived from the `beervolume` feature group in order to have better predictions later on.\n",
        "\n",
        "We retrieve bother feature groups:"
      ],
      "metadata": {
        "id": "GuLwFF462Ln6"
      },
      "id": "GuLwFF462Ln6"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b9c6c942",
      "metadata": {
        "id": "b9c6c942"
      },
      "outputs": [],
      "source": [
        "# Retrieve feature groups: synthetic and engineered\n",
        "beervolume_fg = fs.get_feature_group(\n",
        "    name='beervolume',\n",
        "    version=1,\n",
        ")\n",
        "beervolume_averages_fg = fs.get_feature_group(\n",
        "    name='beervolume_averages',\n",
        "    version=1,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7e72db48",
      "metadata": {
        "id": "7e72db48"
      },
      "source": [
        "## <span style=\"color:#ff5f27\">üî™ Preparing our Training pipeline: Feature Selection </span>\n",
        "Using the Hopsworks feature store, we can very easily select the specific subset that we need for our training pipeline, and be confident that we will be using the correct training dataset in our Machine Learning process:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0740ac7f",
      "metadata": {
        "id": "0740ac7f"
      },
      "outputs": [],
      "source": [
        "# Select features for training dataset\n",
        "query = beervolume_fg.select_all() \\\n",
        "    .join(beervolume_averages_fg.select_except(['date']))\n",
        "query.show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1334a4cb",
      "metadata": {
        "id": "1334a4cb"
      },
      "source": [
        "## <span style=\"color:#ff5f27\">ü§ñ Preparing our Training pipeline: Transformation Functions </span>\n",
        "Using the Hopsworks feature store, we can easily apply a set of transformation functions that will apply to our training pipeline:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bccb7d00",
      "metadata": {
        "id": "bccb7d00"
      },
      "outputs": [],
      "source": [
        "# Load transformation function\n",
        "min_max_scaler = fs.get_transformation_function(name=\"min_max_scaler\")\n",
        "\n",
        "# Define a list of feature names\n",
        "feature_names = [\n",
        "    'ma_7', 'ma_14', 'ma_30', 'daily_rate_of_change', 'volatility_30_day', 'ema_02', 'ema_05', 'rsi'\n",
        "]\n",
        "\n",
        "# Map features to transformations\n",
        "transformation_functions = {\n",
        "    feature_name: min_max_scaler\n",
        "    for feature_name in feature_names\n",
        "}\n",
        "transformation_functions"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "925ad679",
      "metadata": {
        "id": "925ad679"
      },
      "source": [
        "## <span style=\"color:#ff5f27\">‚öôÔ∏è Feature View Creation leveraging feature groups, feature selection and feature transformations </span>\n",
        "Using the feature groups, the selection that we made above and finally the associated transformation functions, we can create a **feature view** in the Hopsworks feature store:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ffdfda5e",
      "metadata": {
        "id": "ffdfda5e"
      },
      "outputs": [],
      "source": [
        "# Get or create the 'beervolume_fv' feature view\n",
        "feature_view = fs.get_or_create_feature_view(\n",
        "    name='beervolume_fv',\n",
        "    version=1,\n",
        "    query=query,\n",
        "    labels=[\"beervolume\"],\n",
        "    transformation_functions=transformation_functions,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3e1e1170",
      "metadata": {
        "id": "3e1e1170"
      },
      "source": [
        "# <span style=\"color:#ff5f27\">üèãÔ∏è Training Dataset Creation based on the Feature View</span>\n",
        "The next part of our process will focus on the creation of a Training dataset based on the feature view that we have prepared above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "554cbace",
      "metadata": {
        "id": "554cbace"
      },
      "outputs": [],
      "source": [
        "# Get training and testing sets\n",
        "X_train, X_test, y_train, y_test = feature_view.train_test_split(\n",
        "    description='Beer Volume Dataset',  # Provide a description for the dataset split\n",
        "    train_start='2023-09-01',      # Start date for the training set\n",
        "    train_end='2023-11-01',        # End date for the training set\n",
        "    test_start='2023-11-01',       # Start date for the testing set\n",
        "    test_end=datetime.today().strftime(\"%Y-%m-%d\"),  # End date for the testing set (current date)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "041666ce",
      "metadata": {
        "id": "041666ce"
      },
      "outputs": [],
      "source": [
        "X_train.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d7315f14",
      "metadata": {
        "id": "d7315f14"
      },
      "outputs": [],
      "source": [
        "y_train.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af817399",
      "metadata": {
        "id": "af817399"
      },
      "outputs": [],
      "source": [
        "# Sort the training features by the 'date' column\n",
        "X_train = X_train.sort_values(\"date\")\n",
        "\n",
        "# Reindex the target 'y_train' to match the sorted order of 'X_train'\n",
        "y_train = y_train.reindex(X_train.index)\n",
        "\n",
        "# Sort the testing features by the 'date' column\n",
        "X_test = X_test.sort_values(\"date\")\n",
        "\n",
        "# Reindex the target 'y_test' to match the sorted order of 'X_test'\n",
        "y_test = y_test.reindex(X_test.index)\n",
        "\n",
        "# Extract and store the 'date' column as a separate DataFrame for both training and testing sets\n",
        "train_date = pd.DataFrame(X_train.pop(\"date\"))\n",
        "test_date = pd.DataFrame(X_test.pop(\"date\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4e7d0253",
      "metadata": {
        "id": "4e7d0253"
      },
      "source": [
        "# <span style=\"color:#ff5f27\">üß¨ Using the training dataset to create a Machine Learning Model</span>\n",
        "\n",
        "In order to now create a predictive model out of our data, we will use the **XGBoost Regressor** to examine the training dataset and figure out a predictive model. XGBoost regressor is a powerful and highly effective machine learning algorithm for regression problems. XGBoost is known for its ability to handle complex relationships in the data, handle missing values, and provide accurate predictions. It's a popular choice in the data science community due to its robustness and excellent predictive performance, making it well-suited for our specific problem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a999d119",
      "metadata": {
        "id": "a999d119"
      },
      "outputs": [],
      "source": [
        "# Initialize the XGBoost regressor\n",
        "model = xgb.XGBRegressor()\n",
        "\n",
        "# Train the model on the training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the validation set\n",
        "y_test_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate RMSE on the validation set\n",
        "mse = mean_squared_error(y_test, y_test_pred, squared=False)\n",
        "print(f\"Mean Squared Error (MSE): {mse}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b0f556d",
      "metadata": {
        "id": "3b0f556d"
      },
      "outputs": [],
      "source": [
        "# Make predictions for a specific ID (ID=1) using the 'predict_id' function\n",
        "prediction_for_id = predict_id(\n",
        "    1,\n",
        "    X_test,\n",
        "    model,\n",
        ")\n",
        "\n",
        "# Generate a Plotly figure for visualizing the predictions\n",
        "fig = plot_prediction_test(\n",
        "    1,\n",
        "    X_train,\n",
        "    X_test,\n",
        "    y_train,\n",
        "    y_test,\n",
        "    train_date,\n",
        "    test_date,\n",
        "    prediction_for_id,\n",
        ")\n",
        "\n",
        "# Display the generated Plotly figure\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "558799fa",
      "metadata": {
        "id": "558799fa"
      },
      "source": [
        "## <span style=\"color:#ff5f27\">‚öôÔ∏è Creating the Model Schema </span>\n",
        "Now that we know what the input training features and the target/output variables will look like, we can create a schema for this and save this to a dictionary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b490e4d",
      "metadata": {
        "id": "3b490e4d"
      },
      "outputs": [],
      "source": [
        "from hsml.schema import Schema\n",
        "from hsml.model_schema import ModelSchema\n",
        "\n",
        "# Create an input schema using the training features\n",
        "input_schema = Schema(X_train.values)\n",
        "\n",
        "# Create an output schema using the target variable\n",
        "output_schema = Schema(y_train)\n",
        "\n",
        "# Create a model schema using the input and output schemas\n",
        "model_schema = ModelSchema(\n",
        "    input_schema=input_schema,\n",
        "    output_schema=output_schema,\n",
        ")\n",
        "\n",
        "# Convert the model schema to a dictionary\n",
        "model_schema.to_dict()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "44e8f511",
      "metadata": {
        "id": "44e8f511"
      },
      "source": [
        "## <span style=\"color:#ff5f27\">üìù Registering the model in the Hopsworks registry</span>\n",
        "We will also save and persist the model in Hopsworks, in case future decisions would need to be retraced to a specific version of the prediction system:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb397a64",
      "metadata": {
        "id": "fb397a64"
      },
      "outputs": [],
      "source": [
        "# Specify the directory for saving the model\n",
        "model_dir = \"beervolume_model\"\n",
        "\n",
        "# Check if the directory exists, and create it if not\n",
        "if not os.path.isdir(model_dir):\n",
        "    os.mkdir(model_dir)\n",
        "\n",
        "# Save the trained XGBoost model using joblib\n",
        "joblib.dump(model, f'{model_dir}/xgboost_beervolume_model.pkl')\n",
        "\n",
        "# Write the generated Plotly figure image to the specified directory\n",
        "fig.write_image(f'{model_dir}/model_prediction.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "782f9731",
      "metadata": {
        "id": "782f9731"
      },
      "outputs": [],
      "source": [
        "# Get the model registry from the project\n",
        "mr = project.get_model_registry()\n",
        "\n",
        "# Create a Python model in the model registry named 'xgboost_price_model'\n",
        "beervolume_model = mr.python.create_model(\n",
        "    name=\"xgboost_beervolume_model\",\n",
        "    metrics={\"MSE\": mse},           # Specify metrics (Mean Squared Error)\n",
        "    model_schema=model_schema,      # Provide the model schema\n",
        "    input_example=X_train.sample(), # Provide an example of the input data\n",
        "    description=\"Beervolume Predictor\",  # Add a description for the model\n",
        ")\n",
        "\n",
        "# Save the model to the specified directory\n",
        "beervolume_model.save(model_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "88bfb99f",
      "metadata": {
        "id": "88bfb99f"
      },
      "source": [
        "# <span style=\"color:#ff5f27\">üöÄ Beervolume Consumption Prediction Model Deployment</span>\n",
        "\n",
        "**About Model Serving**\n",
        "\n",
        "Models can be served via KFServing or \"default\" serving, which means a Docker container exposing a Flask server. For KFServing models, or models written in Tensorflow, we do not need to write a prediction file. However, for sklearn models using default serving, you do need to proceed to write a prediction file.\n",
        "\n",
        "We will not go into this at this point - and wrap up the 2nd part of our prediction system with this training example.\n",
        "\n",
        "----"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4017c8e4",
      "metadata": {
        "id": "4017c8e4"
      },
      "source": [
        "# OPTIONAL!!!\n",
        "## <span style=\"color:#ff5f27\">üìé Predictor script for Python models</span>\n",
        "\n",
        "Scikit-learn and XGBoost models are deployed as Python models, in which case you need to provide a Predict class that implements the predict method. The `predict()` method invokes the model on the inputs and returns the prediction as a list.\n",
        "\n",
        "The `init()` method is run when the predictor is loaded into memory, loading the model from the local directory it is materialized to, ARTIFACT_FILES_PATH.\n",
        "\n",
        "The directive **\"%%writefile\"** writes out the cell before to the given Python file. We will use the **predict_example.py** file to create a deployment for our model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "79a97ca9",
      "metadata": {
        "id": "79a97ca9"
      },
      "outputs": [],
      "source": [
        "%%writefile predict_example.py\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import hsfs\n",
        "import joblib\n",
        "\n",
        "\n",
        "class Predict(object):\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\" Initializes the serving state, reads a trained model\"\"\"\n",
        "        # get feature store handle\n",
        "        fs_conn = hsfs.connection()\n",
        "        self.fs = fs_conn.get_feature_store()\n",
        "\n",
        "        # get feature view\n",
        "        self.fv = self.fs.get_feature_view(\"beervolume_fv\", 1)\n",
        "\n",
        "        # initialize serving\n",
        "        self.fv.init_serving(1)\n",
        "\n",
        "        # load the trained model\n",
        "        self.model = joblib.load(os.environ[\"ARTIFACT_FILES_PATH\"] + \"/xgboost_beervolume_model.pkl\")\n",
        "        print(\"Initialization of Beervolume model Complete\")\n",
        "\n",
        "\n",
        "    def predict(self, id_value):\n",
        "        \"\"\" Serves a Beervolume prediction request usign a trained model\"\"\"\n",
        "        # Retrieve feature vectors\n",
        "        feature_vector = self.fv.get_feature_vector(\n",
        "            entry = {'id': id_value[0]}\n",
        "        )\n",
        "        return self.model.predict(np.asarray(feature_vector[1:]).reshape(1, -1)).tolist()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b979582c",
      "metadata": {
        "id": "b979582c"
      },
      "source": [
        "This script needs to be put into a known location in the Hopsworks file system. Let's call the file predict_example.py and put it in the Models directory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a64048e9",
      "metadata": {
        "id": "a64048e9"
      },
      "outputs": [],
      "source": [
        "# Get the dataset API from the project\n",
        "dataset_api = project.get_dataset_api()\n",
        "\n",
        "# Upload the file \"predict_example.py\" to the \"Models\" dataset, overwriting if it already exists\n",
        "uploaded_file_path = dataset_api.upload(\"predict_example.py\", \"Models\", overwrite=True)\n",
        "\n",
        "# Create the full path to the uploaded predictor script\n",
        "predictor_script_path = os.path.join(\"/Projects\", project.name, uploaded_file_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d7cea10",
      "metadata": {
        "id": "7d7cea10"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cf59ec95",
      "metadata": {
        "id": "cf59ec95"
      },
      "source": [
        "## <span style=\"color:#ff5f27\">üöÄ Create the deployment</span>\n",
        "\n",
        "Here, you fetch the model you want from the model registry and define a configuration for the deployment. For the configuration, you need to specify the serving type (default or KFserving)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f98e2dd",
      "metadata": {
        "id": "9f98e2dd"
      },
      "outputs": [],
      "source": [
        "# Deploy the 'beervolume_model'\n",
        "deployment = beervolume_model.deploy(\n",
        "    name=\"beervolumeonlinemodeldeployment\",  # Specify the deployment name\n",
        "    script_file=predictor_script_path,  # Provide the path to the predictor script\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "86154683",
      "metadata": {
        "id": "86154683"
      },
      "outputs": [],
      "source": [
        "# Start the deployment and wait for it up to 180 seconds\n",
        "deployment.start(await_running=180)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80e5b6ec",
      "metadata": {
        "id": "80e5b6ec"
      },
      "outputs": [],
      "source": [
        "# Get the current state of the deployment and describe it\n",
        "deployment_state = deployment.get_state().describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ea906d47",
      "metadata": {
        "id": "ea906d47"
      },
      "outputs": [],
      "source": [
        "# Predict price for the 1 ID\n",
        "deployment.predict({'instances': [1]})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1354c27a",
      "metadata": {
        "id": "1354c27a"
      },
      "source": [
        "---"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}