{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"font-width:bold; font-size: 3rem; color:#1EB182;\"> **Hopsworks Feature Store** </span><span style=\"font-width:bold; font-size: 3rem; color:#333;\">- Part 03: Online Inference</span>\n",
    "\n",
    "<span style=\"font-width:bold; font-size: 1.4rem;\">In this last notebook you will use your deployment for online inference.  </span>\n",
    "\n",
    "## **üóíÔ∏è This notebook is divided into the following sections:** \n",
    "1. **Deployment Retrieval**: Retrieve your deployment from the model registry.\n",
    "2. **Prediction using deployment**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27;\"> üì° Connecting to Hopsworks Feature Store </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected. Call `.close()` to terminate connection gracefully.\n",
      "Connected. Call `.close()` to terminate connection gracefully.\n"
     ]
    }
   ],
   "source": [
    "# import hopsworks\n",
    "\n",
    "# project = hopsworks.login()\n",
    "\n",
    "# # Get the feature store handle for the project's feature store\n",
    "# fs = project.get_feature_store()\n",
    "\n",
    "import hopsworks\n",
    "conn = hopsworks.connection(\n",
    "    host=\"172f2800-9e76-11ee-ba4c-277d56d9f8e7.cloud.hopsworks.ai\",                                # DNS of your Feature Store instance\n",
    "    hostname_verification=False,                     # Disable for self-signed certificates\n",
    "    api_key_value=\"FQBJQoMsRTlF8st8.x6KJL56DxrR29B4Qkqwux7wF8Nn9ez32u1ELMJNMId9cMLzcXTKkG2aU9bfZlaUu\"          # Feature store API key value \n",
    ")\n",
    "project = conn.get_project('RixNeo4jEmbeddings') # specify your project name\n",
    "fs = project.get_feature_store()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27;\">üóÑ Model Registry</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected. Call `.close()` to terminate connection gracefully.\n"
     ]
    }
   ],
   "source": [
    "# Get the Model Registry\n",
    "mr = project.get_model_registry()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the \"aml_model\" from the model registry\n",
    "model = mr.get_model(\n",
    "    name=\"aml_model\", \n",
    "    version=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='color:#ff5f27'>‚öôÔ∏è Fetch Deployment</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected. Call `.close()` to terminate connection gracefully.\n",
      "Deployment is in failed state. Failed to run: transformer terminated unsuccessfully\n"
     ]
    }
   ],
   "source": [
    "# Access the Model Serving\n",
    "ms = project.get_model_serving()\n",
    "\n",
    "# Specify the deployment name\n",
    "deployment_name = \"amlmodeldeployment\"\n",
    "\n",
    "# Get the deployment with the specified name\n",
    "deployment = ms.get_deployment(deployment_name)\n",
    "\n",
    "# Start the deployment and wait for it to be in a running state for up to 300 seconds\n",
    "deployment.start(await_running=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='color:#ff5f27'>üîÆ Predicting using deployment</span>\n",
    "\n",
    "\n",
    "Finally you can start making predictions with your model!\n",
    "\n",
    "Send inference requests to the deployed model as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModelServingException",
     "evalue": "Deployment not created or running. If it is already created, start it by using `.start()` or check its status with .get_state()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRestAPIError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m~/miniforge3/envs/rixtensorflowmanaged/lib/python3.8/site-packages/hsml/engine/serving_engine.py:185\u001b[0m, in \u001b[0;36mServingEngine.predict\u001b[0;34m(self, deployment_instance, data, inputs)\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_serving_api\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_inference_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdeployment_instance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpayload\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthrough_hopsworks\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RestAPIError \u001b[38;5;28;01mas\u001b[39;00m re:\n",
      "File \u001b[0;32m~/miniforge3/envs/rixtensorflowmanaged/lib/python3.8/site-packages/hsml/core/serving_api.py:231\u001b[0m, in \u001b[0;36mServingApi.send_inference_request\u001b[0;34m(self, deployment_instance, data, through_hopsworks)\u001b[0m\n\u001b[1;32m    228\u001b[0m         path_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_hopsworks_inference_path(\n\u001b[1;32m    229\u001b[0m             _client\u001b[38;5;241m.\u001b[39m_project_id, deployment_instance\n\u001b[1;32m    230\u001b[0m         )\n\u001b[0;32m--> 231\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPOST\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    233\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/rixtensorflowmanaged/lib/python3.8/site-packages/hsml/decorators.py:35\u001b[0m, in \u001b[0;36mconnected.<locals>.if_connected\u001b[0;34m(inst, *args, **kwargs)\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NoHopsworksConnectionError\n\u001b[0;32m---> 35\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43minst\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/rixtensorflowmanaged/lib/python3.8/site-packages/hsml/client/base.py:108\u001b[0m, in \u001b[0;36mClient._send_request\u001b[0;34m(self, method, path_params, query_params, headers, data, stream, files)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m--> 108\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mRestAPIError(url, response)\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stream:\n",
      "\u001b[0;31mRestAPIError\u001b[0m: Metadata operation error: (url: http://a315f16919ea24068b0c8ed08148520b-1724498734.eu-west-1.elb.amazonaws.com/v1/models/amlmodeldeployment:predict). Server response: \nHTTP code: 404, HTTP reason: Not Found, body: b''",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mModelServingException\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 7\u001b[0m\n\u001b[1;32m      2\u001b[0m data \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minputs\u001b[39m\u001b[38;5;124m\"\u001b[39m: model\u001b[38;5;241m.\u001b[39minput_example,\n\u001b[1;32m      4\u001b[0m }\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Make predictions using the deployed model\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mdeployment\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/rixtensorflowmanaged/lib/python3.8/site-packages/hsml/deployment.py:200\u001b[0m, in \u001b[0;36mDeployment.predict\u001b[0;34m(self, data, inputs)\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, data: \u001b[38;5;28mdict\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, inputs: \u001b[38;5;28mlist\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    167\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Send inference requests to the deployment.\u001b[39;00m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;124;03m       One of data or inputs parameters must be set. If both are set, inputs will be ignored.\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;124;03m        `dict`. Inference response.\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_serving_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/rixtensorflowmanaged/lib/python3.8/site-packages/hsml/engine/serving_engine.py:194\u001b[0m, in \u001b[0;36mServingEngine.predict\u001b[0;34m(self, deployment_instance, data, inputs)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RestAPIError \u001b[38;5;28;01mas\u001b[39;00m re:\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    190\u001b[0m         re\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m RestAPIError\u001b[38;5;241m.\u001b[39mSTATUS_CODE_NOT_FOUND\n\u001b[1;32m    191\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m re\u001b[38;5;241m.\u001b[39merror_code\n\u001b[1;32m    192\u001b[0m         \u001b[38;5;241m==\u001b[39m ModelServingException\u001b[38;5;241m.\u001b[39mERROR_CODE_DEPLOYMENT_NOT_RUNNING\n\u001b[1;32m    193\u001b[0m     ):\n\u001b[0;32m--> 194\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ModelServingException(\n\u001b[1;32m    195\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDeployment not created or running. If it is already created, start it by using `.start()` or check its status with .get_state()\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    196\u001b[0m         )\n\u001b[1;32m    198\u001b[0m     re\u001b[38;5;241m.\u001b[39margs \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    199\u001b[0m         re\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m Check the model server logs by using `.get_logs()`\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    200\u001b[0m     )\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m re\n",
      "\u001b[0;31mModelServingException\u001b[0m: Deployment not created or running. If it is already created, start it by using `.start()` or check its status with .get_state()"
     ]
    }
   ],
   "source": [
    "# Prepare input data using the input example from the model\n",
    "data = {\n",
    "    \"inputs\": model.input_example,\n",
    "}\n",
    "\n",
    "# Make predictions using the deployed model\n",
    "predictions = deployment.predict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'predictions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Show the predictions\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mpredictions\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'predictions' is not defined"
     ]
    }
   ],
   "source": [
    "# Show the predictions\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#deployment.get_logs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModelServingException",
     "evalue": "Deployment not created or running. If it is already created, start it by using `.start()` or check its status with .get_state()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRestAPIError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m~/miniforge3/envs/rixtensorflowmanaged/lib/python3.8/site-packages/hsml/engine/serving_engine.py:185\u001b[0m, in \u001b[0;36mServingEngine.predict\u001b[0;34m(self, deployment_instance, data, inputs)\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_serving_api\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_inference_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdeployment_instance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpayload\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthrough_hopsworks\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RestAPIError \u001b[38;5;28;01mas\u001b[39;00m re:\n",
      "File \u001b[0;32m~/miniforge3/envs/rixtensorflowmanaged/lib/python3.8/site-packages/hsml/core/serving_api.py:231\u001b[0m, in \u001b[0;36mServingApi.send_inference_request\u001b[0;34m(self, deployment_instance, data, through_hopsworks)\u001b[0m\n\u001b[1;32m    228\u001b[0m         path_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_hopsworks_inference_path(\n\u001b[1;32m    229\u001b[0m             _client\u001b[38;5;241m.\u001b[39m_project_id, deployment_instance\n\u001b[1;32m    230\u001b[0m         )\n\u001b[0;32m--> 231\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPOST\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    233\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/rixtensorflowmanaged/lib/python3.8/site-packages/hsml/decorators.py:35\u001b[0m, in \u001b[0;36mconnected.<locals>.if_connected\u001b[0;34m(inst, *args, **kwargs)\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NoHopsworksConnectionError\n\u001b[0;32m---> 35\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43minst\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/rixtensorflowmanaged/lib/python3.8/site-packages/hsml/client/base.py:108\u001b[0m, in \u001b[0;36mClient._send_request\u001b[0;34m(self, method, path_params, query_params, headers, data, stream, files)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m--> 108\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mRestAPIError(url, response)\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stream:\n",
      "\u001b[0;31mRestAPIError\u001b[0m: Metadata operation error: (url: http://a315f16919ea24068b0c8ed08148520b-1724498734.eu-west-1.elb.amazonaws.com/v1/models/amlmodeldeployment:predict). Server response: \nHTTP code: 404, HTTP reason: Not Found, body: b''",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mModelServingException\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m node_id \u001b[38;5;129;01min\u001b[39;00m ids_to_score:\n\u001b[1;32m     14\u001b[0m     data \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minputs\u001b[39m\u001b[38;5;124m\"\u001b[39m: [node_id]}\n\u001b[0;32m---> 15\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m anomaly score for node_id \u001b[39m\u001b[38;5;124m\"\u001b[39m, node_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m : \u001b[39m\u001b[38;5;124m\"\u001b[39m,   \u001b[43mdeployment\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutputs\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[0;32m~/miniforge3/envs/rixtensorflowmanaged/lib/python3.8/site-packages/hsml/deployment.py:200\u001b[0m, in \u001b[0;36mDeployment.predict\u001b[0;34m(self, data, inputs)\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, data: \u001b[38;5;28mdict\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, inputs: \u001b[38;5;28mlist\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    167\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Send inference requests to the deployment.\u001b[39;00m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;124;03m       One of data or inputs parameters must be set. If both are set, inputs will be ignored.\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;124;03m        `dict`. Inference response.\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_serving_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/rixtensorflowmanaged/lib/python3.8/site-packages/hsml/engine/serving_engine.py:194\u001b[0m, in \u001b[0;36mServingEngine.predict\u001b[0;34m(self, deployment_instance, data, inputs)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RestAPIError \u001b[38;5;28;01mas\u001b[39;00m re:\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    190\u001b[0m         re\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m RestAPIError\u001b[38;5;241m.\u001b[39mSTATUS_CODE_NOT_FOUND\n\u001b[1;32m    191\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m re\u001b[38;5;241m.\u001b[39merror_code\n\u001b[1;32m    192\u001b[0m         \u001b[38;5;241m==\u001b[39m ModelServingException\u001b[38;5;241m.\u001b[39mERROR_CODE_DEPLOYMENT_NOT_RUNNING\n\u001b[1;32m    193\u001b[0m     ):\n\u001b[0;32m--> 194\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ModelServingException(\n\u001b[1;32m    195\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDeployment not created or running. If it is already created, start it by using `.start()` or check its status with .get_state()\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    196\u001b[0m         )\n\u001b[1;32m    198\u001b[0m     re\u001b[38;5;241m.\u001b[39margs \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    199\u001b[0m         re\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m Check the model server logs by using `.get_logs()`\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    200\u001b[0m     )\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m re\n",
      "\u001b[0;31mModelServingException\u001b[0m: Deployment not created or running. If it is already created, start it by using `.start()` or check its status with .get_state()"
     ]
    }
   ],
   "source": [
    "# Now lets test feature vectors from online store\n",
    "ids_to_score = [\n",
    "    \"0016359b\", \n",
    "    \"0054a022\", \n",
    "    \"00d6b609\", \n",
    "    \"00e14860\", \n",
    "    \"014ed5cb\", \n",
    "    \"01ce3306\", \n",
    "    \"01fa1d01\", \n",
    "    \"04b23f4b\",\n",
    "]\n",
    "\n",
    "for node_id in ids_to_score:\n",
    "    data = {\"inputs\": [node_id]}\n",
    "    print(\" anomaly score for node_id \", node_id, \" : \",   deployment.predict(data)[\"outputs\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> For trouble shooting one can use `get_logs` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#deployment.get_logs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop Deployment\n",
    "To stop the deployment you simply run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27;\"> üéÅ Wrapping things up </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this module you perforemed feature engineering, created feature view and traning dataset, trained advesarial anomaly detection model and depoyed it in production. To setup this pipeline in your enterprise settings contuct us.\n",
    "\n",
    "<img src=\"images/contuct_us.png\" width=\"400px\"></img>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
