{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"font-width:bold; font-size: 3rem; color:#1EB182;\"> **Hopsworks Feature Store** </span><span style=\"font-width:bold; font-size: 3rem; color:#333;\">- Part 02: Model training</span>\n",
    "\n",
    "<span style=\"font-width:bold; font-size: 1.4rem;\"> This notebook explains how to read from a feature group and create training dataset within the feature store. You will train a model on the created training dataset. You will train your model using TensorFlow, although it could just as well be trained with other machine learning frameworks such as Scikit-learn, Keras, and PyTorch. You will also see some of the exploration that can be done in Hopsworks, notably the search functions and the lineage.</span>\n",
    "\n",
    "## **üóíÔ∏è This notebook is divided into the following steps:** \n",
    "\n",
    "1. **Feature Selection**: Select the features you want to train your model on.\n",
    "2. **Feature Transformation**: How the features should be preprocessed.\n",
    "3. **Training Dataset Creation**: Create a dataset for training anomaly detection model.\n",
    "2. **Model Training**: Train your anomaly detection model.\n",
    "3. **Model Registry**: Register model to Hopsworks model registry.\n",
    "4. **Model Deployment**: Deploy the model for real-time inference.\n",
    "\n",
    "![tutorial-flow](../../images/02_training-dataset.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='color:#ff5f27'> üìù Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "from anomaly_detection import GanEncAnomalyDetector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27;\"> üì° Connecting to Hopsworks Feature Store </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected. Call `.close()` to terminate connection gracefully.\n",
      "Connected. Call `.close()` to terminate connection gracefully.\n"
     ]
    }
   ],
   "source": [
    "# import hopsworks\n",
    "\n",
    "# project = hopsworks.login()\n",
    "\n",
    "# fs = project.get_feature_store()\n",
    "\n",
    "import hopsworks\n",
    "conn = hopsworks.connection(\n",
    "    host=\"snurran.hops.works\",                                # DNS of your Feature Store instance\n",
    "    hostname_verification=False,                     # Disable for self-signed certificates\n",
    "    api_key_value=\"mHcmlu3RlZY6VqRu.loXm26Aq1TaygwyS8wb4qrK90Fc0Rmzve1ziE7FresJEgf0DxFHfIUOMthohuh52\"          # Feature store API key value \n",
    ")\n",
    "project = conn.get_project('GraphEmbeddingsDemo') # specify your project name\n",
    "fs = project.get_feature_store()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27;\"> üî™ Feature Selection </span>\n",
    "\n",
    "You start by selecting all the features you want to include for model training/inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve Feature Groups\n",
    "transactions_monthly_fg = fs.get_feature_group(\n",
    "    name=\"transactions_monthly\", \n",
    "    version=1,\n",
    ")\n",
    "\n",
    "graph_embeddings_fg = fs.get_feature_group(\n",
    "    name=\"graph_embeddings\",\n",
    "    version=1,\n",
    ") \n",
    "\n",
    "party_fg = fs.get_feature_group(\n",
    "    name=\"party_labels\", \n",
    "    version=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features for training data\n",
    "selected_features = party_fg.select([\"type\", \"is_sar\"]).join(\n",
    "    transactions_monthly_fg.select(\n",
    "        [\n",
    "            \"monthly_in_count\", \n",
    "            \"monthly_in_total_amount\", \n",
    "            \"monthly_in_mean_amount\", \n",
    "            \"monthly_in_std_amount\", \n",
    "            \"monthly_out_count\", \n",
    "            \"monthly_out_total_amount\", \n",
    "            \"monthly_out_mean_amount\", \n",
    "            \"monthly_out_std_amount\",\n",
    "        ]\n",
    "    )).join(\n",
    "        graph_embeddings_fg.select([\"party_graph_embedding\"]),\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment this if you would like to view your selected features\n",
    "# selected_features.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### <span style=\"color:#ff5f27;\"> ü§ñ Transformation Functions </span>\n",
    "\n",
    "Transformation functions are a mathematical mapping of input data that may be stateful - requiring statistics from the partent feature view (such as number of instances of a category, or mean value of a numerical feature)\n",
    "\n",
    "We will preprocess our data using *min-max scaling* on numerical features and *label encoding* on categorical features. To do this we simply define a mapping between our features and transformation functions. This ensures that transformation functions such as *min-max scaling* are fitted only on the training data (and not the validation/test data), which ensures that there is no data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load built in transformation functions.\n",
    "min_max_scaler = fs.get_transformation_function(name=\"min_max_scaler\")\n",
    "\n",
    "# Map features to transformations.\n",
    "transformation_functions = {\n",
    "    \"monthly_in_count\": min_max_scaler,\n",
    "    \"monthly_in_total_amount\": min_max_scaler,\n",
    "    \"monthly_in_mean_amount\": min_max_scaler,\n",
    "    \"monthly_in_std_amount\": min_max_scaler,\n",
    "    \"monthly_out_count\": min_max_scaler,\n",
    "    \"monthly_out_total_amount\": min_max_scaler,\n",
    "    \"monthly_out_mean_amount\": min_max_scaler,\n",
    "    \"monthly_out_std_amount\": min_max_scaler,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27;\"> ‚öôÔ∏è Feature View Creation </span>\n",
    "\n",
    "In Hopsworks, you write features to feature groups (where the features are stored) and you read features from feature views. A feature view is a logical view over features, stored in feature groups, and a feature view typically contains the features used by a specific model. This way, feature views enable features, stored in different feature groups, to be reused across many different models. The Feature Views allows schema in form of a query with filters, define a model target feature/label and additional transformation functions.\n",
    "In order to create a Feature View we may use `fs.create_feature_view()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature view created successfully, explore it at \n",
      "https://snurran.hops.works:443/p/10375/fs/10322/fv/aml_feature_view/version/1\n"
     ]
    }
   ],
   "source": [
    "# Create the 'aml_feature_view' feature view\n",
    "feature_view = fs.create_feature_view(\n",
    "    name='aml_feature_view',\n",
    "    query=selected_features,\n",
    "    labels=[\"is_sar\"],\n",
    "    transformation_functions=transformation_functions,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternatively instead of creating we can also GET the existing feature view\n",
    "\n",
    "# feature_view = fs.get_feature_view(\n",
    "#     name='aml_feature_view',\n",
    "#     version=1\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27;\"> üèãÔ∏è Training Dataset Creation</span>\n",
    "\n",
    "In Hopsworks training data is a query where the projection (set of features) is determined by the parent FeatureView with an optional snapshot on disk of the data returned by the query.\n",
    "\n",
    "**Training Dataset  may contain splits such as:** \n",
    "* Training set - the subset of training data used to train a model.\n",
    "* Validation set - the subset of training data used to evaluate hparams when training a model\n",
    "* Test set - the holdout subset of training data used to evaluate a mode\n",
    "\n",
    "Training dataset is created using `feature_view.training_data()` method.\n",
    "\n",
    "**From feature view APIs we can also create training datasts based on even time filters specifing `start_time` and `end_time`**. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished: Reading data from Hopsworks, using ArrowFlight (25.48s) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VersionWarning: Incremented version to `1`.\n"
     ]
    }
   ],
   "source": [
    "# Get training data\n",
    "X_train, y_train = feature_view.training_data(\n",
    "    description='AML training dataset'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>monthly_in_count</th>\n",
       "      <th>monthly_in_total_amount</th>\n",
       "      <th>monthly_in_mean_amount</th>\n",
       "      <th>monthly_in_std_amount</th>\n",
       "      <th>monthly_out_count</th>\n",
       "      <th>monthly_out_total_amount</th>\n",
       "      <th>monthly_out_mean_amount</th>\n",
       "      <th>monthly_out_std_amount</th>\n",
       "      <th>party_graph_embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.113415</td>\n",
       "      <td>0.203202</td>\n",
       "      <td>0.159068</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.033702</td>\n",
       "      <td>0.268579</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[-0.9453381299972534, -0.04215634614229202, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.116609</td>\n",
       "      <td>0.185859</td>\n",
       "      <td>0.187547</td>\n",
       "      <td>[-0.446177214384079, -0.019744182005524635, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.027209</td>\n",
       "      <td>0.097499</td>\n",
       "      <td>0.139264</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[0.023978624492883682, 0.00557771185413003, 0....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   type  monthly_in_count  monthly_in_total_amount  monthly_in_mean_amount  \\\n",
       "0     0               0.2                 0.113415                0.203202   \n",
       "1     1               0.0                 0.000000                0.000000   \n",
       "2     0               0.1                 0.027209                0.097499   \n",
       "\n",
       "   monthly_in_std_amount  monthly_out_count  monthly_out_total_amount  \\\n",
       "0               0.159068           0.066667                  0.033702   \n",
       "1               0.000000           0.333333                  0.116609   \n",
       "2               0.139264           0.000000                  0.000000   \n",
       "\n",
       "   monthly_out_mean_amount  monthly_out_std_amount  \\\n",
       "0                 0.268579                0.000000   \n",
       "1                 0.185859                0.187547   \n",
       "2                 0.000000                0.000000   \n",
       "\n",
       "                               party_graph_embedding  \n",
       "0  [-0.9453381299972534, -0.04215634614229202, 0....  \n",
       "1  [-0.446177214384079, -0.019744182005524635, 0....  \n",
       "2  [0.023978624492883682, 0.00557771185413003, 0....  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Displaying the first three rows of the training data\n",
    "X_train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_sar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   is_sar\n",
       "0       0\n",
       "1       0\n",
       "2       0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Displaying the first three rows of the target data\n",
    "y_train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tmp = X_train\n",
    "y_tmp = y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:#ff5f27;\">ü§ñ Model Building</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting string representations of Python literals in 'graph_embeddings' column to actual objects\n",
    "#X_train['party_graph_embedding'] = X_train['party_graph_embedding'].apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert each element in the 'graph_embeddings' column to a NumPy array\n",
    "X_train['party_graph_embedding'] = X_train['party_graph_embedding'].apply(np.array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>monthly_in_count</th>\n",
       "      <th>monthly_in_total_amount</th>\n",
       "      <th>monthly_in_mean_amount</th>\n",
       "      <th>monthly_in_std_amount</th>\n",
       "      <th>monthly_out_count</th>\n",
       "      <th>monthly_out_total_amount</th>\n",
       "      <th>monthly_out_mean_amount</th>\n",
       "      <th>monthly_out_std_amount</th>\n",
       "      <th>emb_0</th>\n",
       "      <th>...</th>\n",
       "      <th>emb_6</th>\n",
       "      <th>emb_7</th>\n",
       "      <th>emb_8</th>\n",
       "      <th>emb_9</th>\n",
       "      <th>emb_10</th>\n",
       "      <th>emb_11</th>\n",
       "      <th>emb_12</th>\n",
       "      <th>emb_13</th>\n",
       "      <th>emb_14</th>\n",
       "      <th>emb_15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.113415</td>\n",
       "      <td>0.203202</td>\n",
       "      <td>0.159068</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.033702</td>\n",
       "      <td>0.268579</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.945338</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.023390</td>\n",
       "      <td>0.025416</td>\n",
       "      <td>0.084324</td>\n",
       "      <td>-0.129248</td>\n",
       "      <td>0.199982</td>\n",
       "      <td>0.232421</td>\n",
       "      <td>-0.125192</td>\n",
       "      <td>0.164788</td>\n",
       "      <td>-0.005094</td>\n",
       "      <td>-0.196161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.116609</td>\n",
       "      <td>0.185859</td>\n",
       "      <td>0.187547</td>\n",
       "      <td>-0.446177</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.055836</td>\n",
       "      <td>0.008371</td>\n",
       "      <td>0.040942</td>\n",
       "      <td>-0.033668</td>\n",
       "      <td>0.101801</td>\n",
       "      <td>0.103888</td>\n",
       "      <td>-0.102467</td>\n",
       "      <td>0.118129</td>\n",
       "      <td>0.015196</td>\n",
       "      <td>-0.119756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.027209</td>\n",
       "      <td>0.097499</td>\n",
       "      <td>0.139264</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.023979</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014531</td>\n",
       "      <td>0.026347</td>\n",
       "      <td>0.029183</td>\n",
       "      <td>-0.005772</td>\n",
       "      <td>0.002941</td>\n",
       "      <td>-0.027411</td>\n",
       "      <td>0.013661</td>\n",
       "      <td>0.006741</td>\n",
       "      <td>-0.004065</td>\n",
       "      <td>0.010534</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows √ó 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   type  monthly_in_count  monthly_in_total_amount  monthly_in_mean_amount  \\\n",
       "0     0               0.2                 0.113415                0.203202   \n",
       "1     1               0.0                 0.000000                0.000000   \n",
       "2     0               0.1                 0.027209                0.097499   \n",
       "\n",
       "   monthly_in_std_amount  monthly_out_count  monthly_out_total_amount  \\\n",
       "0               0.159068           0.066667                  0.033702   \n",
       "1               0.000000           0.333333                  0.116609   \n",
       "2               0.139264           0.000000                  0.000000   \n",
       "\n",
       "   monthly_out_mean_amount  monthly_out_std_amount     emb_0  ...     emb_6  \\\n",
       "0                 0.268579                0.000000 -0.945338  ... -0.023390   \n",
       "1                 0.185859                0.187547 -0.446177  ... -0.055836   \n",
       "2                 0.000000                0.000000  0.023979  ...  0.014531   \n",
       "\n",
       "      emb_7     emb_8     emb_9    emb_10    emb_11    emb_12    emb_13  \\\n",
       "0  0.025416  0.084324 -0.129248  0.199982  0.232421 -0.125192  0.164788   \n",
       "1  0.008371  0.040942 -0.033668  0.101801  0.103888 -0.102467  0.118129   \n",
       "2  0.026347  0.029183 -0.005772  0.002941 -0.027411  0.013661  0.006741   \n",
       "\n",
       "     emb_14    emb_15  \n",
       "0 -0.005094 -0.196161  \n",
       "1  0.015196 -0.119756  \n",
       "2 -0.004065  0.010534  \n",
       "\n",
       "[3 rows x 25 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merge the original DataFrame with a DataFrame of exploded embeddings\n",
    "X_train = X_train.merge(\n",
    "    pd.DataFrame(X_train['party_graph_embedding'].to_list()).add_prefix('emb_'), \n",
    "    left_index=True,\n",
    "    right_index=True,\n",
    ").drop('party_graph_embedding', axis=1)\n",
    "\n",
    "# Display the first three rows of the modified DataFrame\n",
    "X_train.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are going to train [gan for anomaly detection](https://arxiv.org/pdf/1905.11034.pdf). During training step  you will provide only features of accounts that have never been reported for suspicios activity.  You will disclose previously reported accounts to the model only in evaluation step.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter non-suspicious transactions from X_train based on y_train values equal to 0\n",
    "non_sar_transactions = X_train[y_train.values == 0]\n",
    "\n",
    "# Drop any rows with missing values from the non-suspicious transactions DataFrame\n",
    "non_sar_transactions = non_sar_transactions.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "type                          int64\n",
       "monthly_in_count            float64\n",
       "monthly_in_total_amount     float64\n",
       "monthly_in_mean_amount      float64\n",
       "monthly_in_std_amount       float64\n",
       "monthly_out_count           float64\n",
       "monthly_out_total_amount    float64\n",
       "monthly_out_mean_amount     float64\n",
       "monthly_out_std_amount      float64\n",
       "emb_0                       float64\n",
       "emb_1                       float64\n",
       "emb_2                       float64\n",
       "emb_3                       float64\n",
       "emb_4                       float64\n",
       "emb_5                       float64\n",
       "emb_6                       float64\n",
       "emb_7                       float64\n",
       "emb_8                       float64\n",
       "emb_9                       float64\n",
       "emb_10                      float64\n",
       "emb_11                      float64\n",
       "emb_12                      float64\n",
       "emb_13                      float64\n",
       "emb_14                      float64\n",
       "emb_15                      float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_sar_transactions.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets define Tensorflow Dataset as we are going to train keras tensorflow model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def windowed_dataset(dataset, window_size, batch_size):\n",
    "    # Create a windowed dataset using the specified window_size and shift of 1\n",
    "    # Drop any remaining elements that do not fit in complete windows\n",
    "    ds = dataset.window(window_size, shift=1, drop_remainder=True)\n",
    "\n",
    "    # Flatten the nested datasets into a single dataset of windows\n",
    "    ds = ds.flat_map(lambda x: x.batch(window_size))\n",
    "\n",
    "    # Batch the windows into batches of the specified batch_size\n",
    "    # Use drop_remainder=True to ensure that all batches have the same size\n",
    "    # Prefetch one batch to improve performance\n",
    "    return ds.batch(batch_size, drop_remainder=True).prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_PrefetchDataset element_spec=TensorSpec(shape=(16, None, 25), dtype=tf.float32, name=None)>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert non_sar_transactions to a TensorFlow dataset, casting the values to float32\n",
    "training_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    tf.cast(non_sar_transactions.astype('float32'), tf.float32)\n",
    ")\n",
    "\n",
    "# Use the windowed_dataset function to create a windowed dataset\n",
    "# Parameters: window_size=2 (sequence length), batch_size=16 (number of sequences in each batch)\n",
    "training_dataset = windowed_dataset(\n",
    "    training_dataset, \n",
    "    window_size=2, \n",
    "    batch_size=16,\n",
    ")\n",
    "\n",
    "training_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27;\"> üèÉ Train Model</span>\n",
    "\n",
    "Next we'll train a model. Here, we set the class weight of the positive class to be twice as big as the negative class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27;\">üß¨ Model architecture</span>\n",
    "\n",
    "Key components:\n",
    "\n",
    "- `Encoder`(encoder_model) takes input data and compresses it into a latent representation. The encoder consists of two Convolutional 1D layers with Batch Normalization and Leaky ReLU activation functions.\n",
    "\n",
    "- `Generator`(generator_model) takes a latent vector and generates synthetic data. The generator consists of two Convolutional 1D layers with Batch Normalization and Leaky ReLU activation functions. The last layer produces data with the same shape as the input data.\n",
    "\n",
    "- `Discriminator`(discriminator_model) distinguishes between real and generated (fake) data. It comprises two Convolutional 1D layers with Batch Normalization and Leaky ReLU activation functions, followed by a fully connected layer. The output is a single value representing the probability that the input is real.\n",
    "\n",
    "![tutorial-flow](images/model_architecture.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\n"
     ]
    }
   ],
   "source": [
    "# Create an instance of the GanEncAnomalyDetector model with input dimensions [2, n_features]\n",
    "model = GanEncAnomalyDetector([2, training_dataset.element_spec.shape[-1]])\n",
    "\n",
    "# Compile the model\n",
    "model.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder_model (None, 1, 25)\n",
      "generator_model (None, 2, 25)\n",
      "discriminator_model (None, 1)\n"
     ]
    }
   ],
   "source": [
    "# Iterate through each layer in the model\n",
    "for layer in model.layers:\n",
    "    # Print the name and output shape of each layer\n",
    "    print(layer.name, layer.output_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-11 16:07:31.970323: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-04-11 16:08:36.008588: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 4543881861083583095\n",
      "2024-04-11 16:08:36.008602: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 4038103705195873241\n",
      "2024-04-11 16:08:36.008607: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 7007604288624178269\n",
      "2024-04-11 16:08:36.008631: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 12774717217536694027\n",
      "2024-04-11 16:08:36.008635: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 17693546468944701611\n",
      "2024-04-11 16:08:36.008638: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 14878842910469654707\n",
      "2024-04-11 16:08:36.008648: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 14804039036333766973\n",
      "2024-04-11 16:08:36.008651: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 10740640697407495435\n",
      "2024-04-11 16:08:36.008654: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 16996776834220019681\n",
      "2024-04-11 16:08:36.008659: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 1683186886782976458\n",
      "2024-04-11 16:08:36.008662: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 16654800703966717566\n",
      "2024-04-11 16:08:36.008665: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 17822057444253034432\n",
      "2024-04-11 16:08:36.008667: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 1044590777411823366\n",
      "2024-04-11 16:08:36.008670: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 10262234599424788170\n",
      "2024-04-11 16:08:36.008673: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 10806415738961528558\n",
      "2024-04-11 16:08:36.008676: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 18412345353663278720\n",
      "2024-04-11 16:08:36.008678: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 8915915739307643086\n",
      "2024-04-11 16:08:36.008681: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 15887263417422762082\n",
      "2024-04-11 16:08:36.008684: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 926465723046376654\n",
      "2024-04-11 16:08:36.008687: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 13890819081187063012\n",
      "2024-04-11 16:08:36.008691: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 14812191187815602648\n",
      "2024-04-11 16:08:36.008694: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 335828931025377612\n",
      "2024-04-11 16:08:36.008697: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 12155544233215548690\n"
     ]
    }
   ],
   "source": [
    "# Train the model using the training_dataset\n",
    "# Set the number of epochs to 2 and suppress verbose output during training\n",
    "history = model.fit(\n",
    "    training_dataset,  # Training dataset used for model training\n",
    "    epochs=2,          # Number of training epochs\n",
    "    verbose=0,         # Verbosity mode (0: silent, 1: progress bar, 2: one line per epoch)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary to store metrics\n",
    "# The key is 'loss', and the value is the initial value of the generator loss from the training history\n",
    "metrics = {\n",
    "    'loss': history.history[\"g_loss\"][0],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#ff5f27;\">‚öôÔ∏è Model Schema</span>\n",
    "\n",
    "The model needs to be set up with a [Model Schema](https://docs.hopsworks.ai/3.0/user_guides/mlops/registry/model_schema/), which describes the inputs and outputs for a model.\n",
    "\n",
    "A Model Schema can be automatically generated from training examples, as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_schema': {'columnar_schema': [{'name': 'type', 'type': 'int64'},\n",
       "   {'name': 'monthly_in_count', 'type': 'float64'},\n",
       "   {'name': 'monthly_in_total_amount', 'type': 'float64'},\n",
       "   {'name': 'monthly_in_mean_amount', 'type': 'float64'},\n",
       "   {'name': 'monthly_in_std_amount', 'type': 'float64'},\n",
       "   {'name': 'monthly_out_count', 'type': 'float64'},\n",
       "   {'name': 'monthly_out_total_amount', 'type': 'float64'},\n",
       "   {'name': 'monthly_out_mean_amount', 'type': 'float64'},\n",
       "   {'name': 'monthly_out_std_amount', 'type': 'float64'},\n",
       "   {'name': 'emb_0', 'type': 'float64'},\n",
       "   {'name': 'emb_1', 'type': 'float64'},\n",
       "   {'name': 'emb_2', 'type': 'float64'},\n",
       "   {'name': 'emb_3', 'type': 'float64'},\n",
       "   {'name': 'emb_4', 'type': 'float64'},\n",
       "   {'name': 'emb_5', 'type': 'float64'},\n",
       "   {'name': 'emb_6', 'type': 'float64'},\n",
       "   {'name': 'emb_7', 'type': 'float64'},\n",
       "   {'name': 'emb_8', 'type': 'float64'},\n",
       "   {'name': 'emb_9', 'type': 'float64'},\n",
       "   {'name': 'emb_10', 'type': 'float64'},\n",
       "   {'name': 'emb_11', 'type': 'float64'},\n",
       "   {'name': 'emb_12', 'type': 'float64'},\n",
       "   {'name': 'emb_13', 'type': 'float64'},\n",
       "   {'name': 'emb_14', 'type': 'float64'},\n",
       "   {'name': 'emb_15', 'type': 'float64'}]},\n",
       " 'output_schema': {'columnar_schema': [{'name': 'is_sar', 'type': 'int64'}]}}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from hsml.schema import Schema\n",
    "from hsml.model_schema import ModelSchema\n",
    "\n",
    "# Define the input schema using the values of X_train\n",
    "input_schema = Schema(X_train)\n",
    "\n",
    "# Define the output schema using y_train\n",
    "output_schema = Schema(y_train)\n",
    "\n",
    "# Create a ModelSchema object specifying the input and output schemas\n",
    "model_schema = ModelSchema(\n",
    "    input_schema=input_schema, \n",
    "    output_schema=output_schema,\n",
    ")\n",
    "\n",
    "# Convert the model schema to a dictionary for further inspection or serialization\n",
    "model_schema.to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27;\">üìù Register model</span>\n",
    "\n",
    "One of the features in Hopsworks is the model registry. This is where we can store different versions of models and compare their performance. Models from the registry can then be served as API endpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting trained model to: aml_model\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <anomaly_detection.GanEncAnomalyDetector object at 0x3858b2850>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <anomaly_detection.GanEncAnomalyDetector object at 0x3858b2850>, because it is not built.\n",
      "INFO:tensorflow:Assets written to: aml_model/assets\n",
      "INFO:tensorflow:Assets written to: aml_model/assets\n",
      "External IP not configured for the Istio ingress gateway, the Hopsworks client will be used for model inference instead\n",
      "Connected. Call `.close()` to terminate connection gracefully.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03aad61d5ef44418bae8ea4c3e36a1e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "021c92a733524246b7cfb99be3737149",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading: 0.000%|          | 0/6148 elapsed<00:00 remaining<?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bec4adb273704f13a566748fa7a65ee7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading: 0.000%|          | 0/57 elapsed<00:00 remaining<?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b25105926f544d6b523ad8957cf64c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading: 0.000%|          | 0/2397674 elapsed<00:00 remaining<?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79831f6b656f4745a64047a28c6288d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading: 0.000%|          | 0/6148 elapsed<00:00 remaining<?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb073daac47f462086406cb2455f35a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading: 0.000%|          | 0/233670 elapsed<00:00 remaining<?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cd7112b0a1d4f959cf66bf9375b0674",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading: 0.000%|          | 0/5498 elapsed<00:00 remaining<?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87877836485f4ea29b1cf633ded748c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading: 0.000%|          | 0/12 elapsed<00:00 remaining<?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b60829cdfd194292b43807f9d4e63fcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading: 0.000%|          | 0/2011 elapsed<00:00 remaining<?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model created, explore it at https://snurran.hops.works:443/p/10375/models/aml_model/1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Model(name: 'aml_model', version: 1)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the path for exporting the trained model\n",
    "export_path = \"aml_model\"\n",
    "print('Exporting trained model to: {}'.format(export_path))\n",
    "\n",
    "# Get the concrete function for serving the model\n",
    "call = model.serve_function.get_concrete_function(tf.TensorSpec([None, None, None], tf.float32))\n",
    "\n",
    "# Save the model to the specified export path with the serving signature\n",
    "tf.saved_model.save(\n",
    "    model, \n",
    "    export_path, \n",
    "    signatures=call,\n",
    ")\n",
    "\n",
    "# Access the model registry in your project\n",
    "\n",
    "mr = project.get_model_registry()\n",
    "\n",
    "# Create a TensorFlow model in the model registry with specified metadata\n",
    "mr_model = mr.tensorflow.create_model(\n",
    "    name=\"aml_model\",                                    # Specify the model name\n",
    "    metrics=metrics,                                     # Include model metrics\n",
    "    model_schema=model_schema,                           # Include model schema\n",
    "    description=\"Adversarial anomaly detection model.\",  # Model description\n",
    "    input_example=[\"70408aef\"],                          # Input example\n",
    ")\n",
    "\n",
    "# Save the registered model to the model registry\n",
    "mr_model.save(export_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27;\"> üöÄ Model Deployment</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting aml_model_transformer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile aml_model_transformer.py\n",
    "\n",
    "import os\n",
    "import hsfs\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "class Transformer(object):\n",
    "    \n",
    "    def __init__(self):        \n",
    "        # Get feature store handle\n",
    "        fs_conn = hsfs.connection()\n",
    "        self.fs = fs_conn.get_feature_store()\n",
    "        \n",
    "        # Get feature views\n",
    "        self.fv = self.fs.get_feature_view(\n",
    "            name=\"aml_feature_view\", \n",
    "            version=1,\n",
    "        )\n",
    "        \n",
    "        # Initialise serving\n",
    "        self.fv.init_serving(1)\n",
    "    \n",
    "    def preprocess(self, inputs):\n",
    "        # Retrieve feature vector using the feature vector provider\n",
    "        feature_vector = self.fv.get_feature_vector({\"id\": inputs[\"inputs\"][0]})\n",
    "\n",
    "        # Explode embeddings (flatten the list of embeddings)\n",
    "        feature_vector_exploded_emb = [*self.flat2gen(feature_vector)]\n",
    "\n",
    "        # Reshape feature vector to match the model's input shape\n",
    "        feature_vector_reshaped = np.array(feature_vector_exploded_emb).reshape(1, 25)\n",
    "\n",
    "        # Convert the feature vector to a TensorFlow constant\n",
    "        input_vector = tf.constant(feature_vector_reshaped, dtype=tf.float32)\n",
    "\n",
    "        # Add a time dimension (axis=1) to the input vector\n",
    "        input_vector = tf.expand_dims(input_vector, axis=1)\n",
    "\n",
    "        # Duplicate the input vector to create a pair\n",
    "        input_vector = tf.tile(input_vector, [1, 2, 1])\n",
    "        \n",
    "        # Duplicate the input vector to create a pair\n",
    "        input_vector = input_vector.numpy().tolist()\n",
    "\n",
    "        # Return the preprocessed input dictionary\n",
    "        return {\n",
    "            'inputs': input_vector\n",
    "        }\n",
    "\n",
    "    def postprocess(self, outputs):\n",
    "        return outputs\n",
    "\n",
    "    def flat2gen(self, alist):\n",
    "        for item in alist:\n",
    "            if isinstance(item, list):\n",
    "                for subitem in item: yield subitem\n",
    "            else:\n",
    "                yield item "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8f68ab9aaab49c9bca5c747570a791f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading: 0.000%|          | 0/1804 elapsed<00:00 remaining<?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from hsml.transformer import Transformer\n",
    "\n",
    "# Get the dataset API from the project\n",
    "dataset_api = project.get_dataset_api()\n",
    "\n",
    "# Upload the transformer script file to the \"Models\" dataset\n",
    "uploaded_file_path = dataset_api.upload(\n",
    "    \"aml_model_transformer.py\",   # Name of the script file\n",
    "    \"Resources\",                     # Destination folder in the dataset\n",
    "    overwrite=True,               # Overwrite the file if it already exists\n",
    ")\n",
    "\n",
    "# Construct the full path to the uploaded transformer script file\n",
    "transformer_script_path = os.path.join(\n",
    "    \"/Projects\", \n",
    "    project.name, \n",
    "    uploaded_file_path,\n",
    ")\n",
    "\n",
    "# Create a Transformer object using the uploaded script\n",
    "transformer_script = Transformer(\n",
    "    script_file=transformer_script_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deployment created, explore it at https://snurran.hops.works:443/p/10375/deployments/7195\n",
      "Before making predictions, start the deployment by using `.start()`\n"
     ]
    }
   ],
   "source": [
    "# Retrieve the \"aml_model\" from the model registry\n",
    "model = mr.get_model(\n",
    "    name=\"aml_model\", \n",
    "    version=1,\n",
    ")\n",
    "\n",
    "# Deploy the model with the specified name (\"amlmodeldeployment\") and associated transformer\n",
    "deployment = model.deploy(\n",
    "    name=\"amlmodeldeployment\",      # Specify the deployment name\n",
    "    transformer=transformer_script, # Associate the transformer script with the deployment\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deployment: amlmodeldeployment\n",
      "{\n",
      "    \"artifact_version\": 1,\n",
      "    \"batching_configuration\": {\n",
      "        \"batching_enabled\": false\n",
      "    },\n",
      "    \"created\": \"2024-04-11T16:10:30.352Z\",\n",
      "    \"creator\": \"Rik Van Bruggen\",\n",
      "    \"description\": null,\n",
      "    \"id\": 7195,\n",
      "    \"inference_logging\": \"NONE\",\n",
      "    \"model_framework\": \"TENSORFLOW\",\n",
      "    \"model_name\": \"aml_model\",\n",
      "    \"model_path\": \"/Projects/GraphEmbeddingsDemo/Models/aml_model\",\n",
      "    \"model_server\": \"TENSORFLOW_SERVING\",\n",
      "    \"model_version\": 1,\n",
      "    \"name\": \"amlmodeldeployment\",\n",
      "    \"predictor\": null,\n",
      "    \"predictor_resources\": {\n",
      "        \"limits\": {\n",
      "            \"cores\": 1.0,\n",
      "            \"gpus\": 0,\n",
      "            \"memory\": 1024\n",
      "        },\n",
      "        \"requests\": {\n",
      "            \"cores\": 0.2,\n",
      "            \"gpus\": 0,\n",
      "            \"memory\": 32\n",
      "        }\n",
      "    },\n",
      "    \"requested_instances\": 0,\n",
      "    \"requested_transformer_instances\": 0,\n",
      "    \"serving_tool\": \"KSERVE\",\n",
      "    \"transformer\": \"aml_model_transformer.py\",\n",
      "    \"transformer_resources\": {\n",
      "        \"limits\": {\n",
      "            \"cores\": 1.0,\n",
      "            \"gpus\": 0,\n",
      "            \"memory\": 1024\n",
      "        },\n",
      "        \"requests\": {\n",
      "            \"cores\": 0.2,\n",
      "            \"gpus\": 0,\n",
      "            \"memory\": 32\n",
      "        }\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(\"Deployment: \" + deployment.name)\n",
    "deployment.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The deployment has now been registered. However, to start it you need to run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3108f8f149be4c3b8d7bbae0f3b35cd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start making predictions by using `.predict()`\n"
     ]
    }
   ],
   "source": [
    "deployment.start(await_running=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> For trouble shooting one can use `get_logs` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment.get_logs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> To stop the deployment you simply run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## <span style=\"color:#ff5f27;\"> ‚è≠Ô∏è Next: Part 03: Online Inference </span>\n",
    "    \n",
    "In the next notebook you will use your deployment for online inference."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
